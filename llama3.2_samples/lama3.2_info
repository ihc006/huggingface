$ apt-get update && apt-get install -y \
    python  \
    python-pip \
    python-dev \
    python-venv \

$ pip install transformers torch sentencepiece Pillow requests scipy pandas numpy 

$ pip install huggingface_hub accelerate bitsandbytes  # kurulmasi tavsiye edilenler

 
pip install setuptools_rust build-essential rustc  --upgrade pip setuptools  -y
 
apt-get update

 
pip install transformers[tf-cpu]


pip install setuptools_rust

 
apt-get update

apt-get install build-essential rustc -y

 
pip install --upgrade pip setuptools

 
pip install transformers[tf-cpu]


###  
python -m venv .env
source .env/bin/activate

pip install transformers
pip install setuptools_rust

pip install 'transformers[torch]'
pip install 'transformers[tf-cpu]'

pip install 'transformers[flax]'

# if ðŸ¤— Transformers has been properly installed by running the following command. It will download a pretrained model:

python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
  # out the label and score:

[{'label': 'POSITIVE', 'score': 0.9998704791069031}]



##################################################################################33


Install from pip

# Install vLLM from pip:
pip install vllm

# Load and run the model:
vllm serve "meta-llama/Llama-3.2-11B-Vision-Instruct"

# Call the server using curl:
curl -X POST "http://localhost:8000/v1/chat/completions" \ 
	-H "Content-Type: application/json" \ 
	--data '{
		"model": "meta-llama/Llama-3.2-11B-Vision-Instruct"
		"messages": [
			{"role": "user", "content": "Hello!"}
		]
	}'
	
	
	Use Docker images

# Deploy with docker on Linux:
docker run --runtime nvidia --gpus all \
	--name my_vllm_container \
	-v ~/.cache/huggingface:/root/.cache/huggingface \
 	--env "HUGGING_FACE_HUB_TOKEN=<secret>" \
	-p 8000:8000 \
	--ipc=host \
	vllm/vllm-openai:latest \
	--model meta-llama/Llama-3.2-11B-Vision-Instruct

# Load and run the model:
docker exec -it my_vllm_container bash -c "vllm serve meta-llama/Llama-3.2-11B-Vision-Instruct"

# Call the server using curl:
curl -X POST "http://localhost:8000/v1/chat/completions" \ 
	-H "Content-Type: application/json" \ 
	--data '{
		"model": "meta-llama/Llama-3.2-11B-Vision-Instruct"
		"messages": [
			{"role": "user", "content": "Hello!"}
		]
	}'



 
